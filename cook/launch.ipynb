{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "载入本地模型",
   "id": "9a8a392f998a6620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T02:29:40.310282Z",
     "start_time": "2025-03-20T02:29:39.146209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Local\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llama2_chat = ChatOllama(model=\"deepseek-r1:14b\")\n",
    "# Simply set the LLM we want to use\n",
    "llm = llama2_chat"
   ],
   "id": "6ba4d0dbd9062ebf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guosh\\AppData\\Local\\Temp\\ipykernel_33872\\1441501452.py:7: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llama2_chat = ChatOllama(model=\"deepseek-r1:14b\")\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T07:14:07.965084Z",
     "start_time": "2025-03-19T07:12:48.048774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Update the template based on the type of SQL Database like MySQL, Microsoft SQL Server and so on\n",
    "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Given an input question, convert it to a SQL query. No pre-amble.\"),\n",
    "        (\"human\", template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain to query\n",
    "\n",
    "\n",
    "sql_response = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "sql_response.invoke({\"question\": \"What team is Klay Thompson on?\"})\n"
   ],
   "id": "b6ba4e6f0bae9680",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, let me try to figure out how to convert the question \"What team is Klay Thompson on?\" into a SQL query based on the provided table schema. \\n\\nFirst, I need to understand the table structure. The tables involved are \\'players\\' and \\'teams\\'. The \\'players\\' table has columns like player_id, name, position, points_per_game, rebounds_per_game, assists_per_game, team_id, and year_joined. The \\'teams\\' table includes team_id, team_name, location, conference, division, and year_founded.\\n\\nThe question is asking about which team Klay Thompson plays for. So, I need to find the team associated with Klay Thompson from the \\'players\\' table. \\n\\nIn SQL terms, this sounds like a JOIN operation between the two tables on the team_id column because each player belongs to a specific team identified by their team_id. Since we\\'re looking for Klay Thompson specifically, I\\'ll use his name in the WHERE clause.\\n\\nI should join the players and teams tables using an INNER JOIN since every player should have a corresponding team. The SELECT statement will fetch the team_name from the \\'teams\\' table where the player\\'s name is Klay Thompson.\\n\\nSo, putting it all together, the SQL query would select the team_name from teams where the players\\' name matches Klay Thompson and join the tables based on team_id.\\n</think>\\n\\nSELECT t.team_name\\nFROM players p\\nINNER JOIN teams t ON p.team_id = t.team_id\\nWHERE p.name = \\'Klay Thompson\\';'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:13:27.415258Z",
     "start_time": "2025-03-19T10:13:21.901412Z"
    }
   },
   "cell_type": "code",
   "source": "llm.invoke('nihao')",
   "id": "bfede821621d3191",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\n你好！有什么我可以帮助你的吗？', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:14b', 'created_at': '2025-03-19T10:13:27.410851Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 5500315600, 'load_duration': 27216400, 'prompt_eval_count': 5, 'prompt_eval_duration': 436000000, 'eval_count': 13, 'eval_duration': 2813000000}, id='run-b60532e7-4dbc-4bce-92d3-a8db728c6e2c-0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "coder\n",
    "\n"
   ],
   "id": "f3b85e41a08a934"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:12:08.321650Z",
     "start_time": "2025-03-19T10:06:12.944295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Update the template based on the type of SQL Database like MySQL, Microsoft SQL Server and so on\n",
    "template = \"\"\"Write code to complete the user's task:\n",
    "Task: {task}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI code bot. Your name is Levy.\"),\n",
    "        (\"human\", template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain to query\n",
    "\n",
    "\n",
    "coder_response = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "coder_response.invoke({\"task\": \"写一段python代码，用langchain实现rag功能?\"})\n"
   ],
   "id": "32380fd45b37246a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n好的，我现在需要帮助用户完成使用LangChain实现RAG（Retrieval-Augmented Generation）功能的Python代码。首先，我得理解什么是RAG。RAG是一种结合了检索和生成模型的技术，通常用于问答系统中，先从文档库中检索相关信息，然后用生成模型根据这些信息回答问题。\\n\\n接下来，用户的需求是写一段Python代码来实现这个功能。那么，我需要考虑使用哪些LangChain的组件。首先，加载文档的部分可能需要用到langchain.document_loaders模块中的某种加载器，比如PDFLoader或者TextLoader，具体取决于用户的文档类型。假设用户有多个文本文件，我可以推荐用DirectoryLoader来批量加载。\\n\\n然后是分块和转换文本。分割长文档是为了提高处理效率，ChunkTransformer可以用来调整每个块的大小。这样处理后的文档会更易于管理，并且检索时能更快地找到相关信息。\\n\\n接下来是构建向量数据库。这里可以选择像FAISS这样的库，因为它适合在内存中进行高效的相似度搜索。将转换后的文本嵌入到向量空间中，然后存储到数据库里，这样后续检索的时候就能快速找到最相关的文档块。\\n\\n然后是实现问答功能。需要定义一个函数，接受问题作为输入，从向量数据库中检索相关文档，并基于这些文档生成答案。这里可能需要用到langchain_core的question_answering模块中的QuestionAnsweringChain，或者自己构建一个生成模型来进行回答。\\n\\n用户还提到要给出使用步骤和代码示例，所以我需要详细说明每个部分的作用，以及如何将它们组合起来。此外，我应该提醒用户安装必要的库，比如langchain、llama-cpp-python、faiss和PyPDF2，这样他们可以顺利运行代码。\\n\\n在代码中，可能需要加载特定的模型，比如LlamaCpp，用于生成回答。同时，在构建向量存储时，确保嵌入维度和其他参数设置正确，以获得良好的检索效果。\\n\\n最后，我应该考虑用户的实际使用场景。也许他们会处理多种格式的文档，或者有特定的问题类型需要优化。因此，代码示例中最好能展示一个完整的流程，并给出一些可扩展的方向，比如更换不同的加载器或生成模型。\\n\\n总结一下，我的步骤是：导入必要的库和模块，加载并分割文档，转换文本块，构建向量数据库，定义问答函数，并提供使用说明和安装指导。这样用户就能根据示例代码快速实现RAG功能了。\\n</think>\\n\\n好的！以下是一个使用 LangChain 实现 RAG（Retrieval-Augmented Generation）功能的 Python 代码示例：\\n\\n### 使用步骤：\\n1. 首先，确保你已经安装了必要的库：\\n```bash\\npip install langchain llama-cpp-python faiss-cpu pyPDF2\\n```\\n\\n2. 准备一些文档（TXT、PDF 或其他格式文件）在 `./docs` 文件夹中。\\n\\n3. 运行代码，并根据提示输入你的问题。\\n\\n### 代码实现：\\n\\n```python\\nimport os\\nfrom langchain.document_loaders import DirectoryLoader\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.embeddings import LlamaCppEmbeddings\\nfrom langchain.vectorstores import FAISS\\nfrom langchain_core.retrieval import VectorStoreRetrieval\\nfrom langchain_core.pipeline import Pipeline, SequentialPipeline\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.chain import LLMChain, Simplequirer\\n\\n# 加载文档\\ndef load_documents():\\n    loader = DirectoryLoader(\"./docs\", glob=\"**\\\\*\")  # 加载所有文件\\n    documents = loader.load()\\n    return documents\\n\\n# 分块和转换文本\\ndef process_documents(documents):\\n    text_splitter = CharacterTextSplitter(\\n        separator=\\'\\\\n\\',\\n        chunk_size=1000,\\n        chunk_overlap=200\\n    )\\n    texts = text_splitter.split_documents(documents)\\n    return texts\\n\\n# 构建向量数据库\\ndef create_vector_store(texts):\\n    embeddings = LlamaCppEmbeddings()  # 使用 Llama 模型进行嵌入\\n    vector_store = FAISS.from_documents(texts, embeddings)\\n    return vector_store\\n\\n# 定义问答函数\\ndef rag_qa(vector_store):\\n    retriever = VectorStoreRetrieval(vector_store)  # 创建检索器\\n    \\n    # 定义提示模板\\n    prompt_template = \"\"\"基于以下上下文回答问题。如果有多个段落，请确保答案包含所有相关段落的信息。\\n    上下文:\\n    {context}\\n    问题: {question}\\n    \"\"\"\\n    qa_prompt = PromptTemplate.from_template(prompt_template)\\n    \\n    # 初始化 LLM（这里使用 LlamaCpp）\\n    llm_chain = LLMChain(llm=LlamaCpp(), prompt=qa_prompt)\\n    \\n    return Simplequirer(retriever=retriever, chain=llm_chain)\\n\\n# 主函数\\ndef main():\\n    documents = load_documents()\\n    texts = process_documents(documents)\\n    vector_store = create_vector_store(texts)\\n    rag = rag_qa(vector_store)\\n    \\n    while True:\\n        question = input(\"请输入你的问题：\")\\n        if not question.strip():\\n            continue\\n        answer = rag.run(question)\\n        print(f\"答案:\\\\n{answer}\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### 代码说明：\\n1. **加载文档**：使用 `DirectoryLoader` 加载指定文件夹中的所有文档。\\n2. **分块和转换文本**：使用 `CharacterTextSplitter` 将长文档分割成小块，便于处理和检索。\\n3. **构建向量数据库**：使用 `LlamaCppEmbeddings` 对文本进行嵌入表示，并存储到 `FAISS` 中，用于高效的相似度搜索。\\n4. **定义问答函数**：\\n   - 使用 `VectorStoreRetrieval` 从向量数据库中检索相关文档。\\n   - 定义一个提示模板，指导生成模型如何基于上下文回答问题。\\n   - 使用 `LLMChain` 和 `Simplequirer` 实现问答功能。\\n\\n### 使用示例：\\n运行代码后，输入你的问题，程序会根据文档内容生成答案。\\n\\n```bash\\npython rag_example.py\\n请输入你的问题：What is the main purpose of LangChain?\\n```\\n\\n希望这段代码能帮助你快速上手使用 LangChain 实现 RAG 功能！如果有其他问题，请随时告诉我。'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:29:19.739461Z",
     "start_time": "2025-03-19T10:28:44.453778Z"
    }
   },
   "cell_type": "code",
   "source": "coder_response.invoke({\"task\": \"what is your name？\"})",
   "id": "3087bdf72691bd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the user asked me what my name is. Let me think about how to respond properly.\\n\\nFirst, I need to acknowledge that they\\'re asking about my identity. My name is Levy, as assigned by the user. I should make sure to mention that I\\'m an AI assistant created by DeepSeek. \\n\\nI also want to offer further assistance, so adding a line like \"How can I assist you today?\" makes sense. It keeps the conversation open and invites them to continue.\\n\\nPutting it all together, the response should be clear, friendly, and helpful.\\n</think>\\n\\nHello! My name is Levy, an AI assistant created by DeepSeek. How can I assist you today?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
