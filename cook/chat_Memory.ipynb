{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load local model",
   "id": "622c0ec788a57a36"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T08:28:09.678673Z",
     "start_time": "2025-03-21T08:28:09.509873Z"
    }
   },
   "source": [
    "# Local\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from libs.core.tests.unit_tests.output_parsers.test_openai_tools import message\n",
    "\n",
    "ds_chat = ChatOllama(model=\"deepseek-r1:7b\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guosh\\AppData\\Local\\Temp\\ipykernel_42856\\3086055942.py:8: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  ds_chat = ChatOllama(model=\"deepseek-r1:7b\")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Maintain message list",
   "id": "8fa7941c88ae1661"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T08:28:31.087952Z",
     "start_time": "2025-03-21T08:28:14.240379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Answer all questions to the best of your ability. Answer in English.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | ds_chat\n",
    "mes_list=    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    "ai_msg = chain.invoke(mes_list)\n",
    "print(ai_msg.content)"
   ],
   "id": "f8f7366b2cec39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, so the user asked me to translate \"I love programming.\" into French earlier, and I responded with \"J'adore la programmation.\" Now they're following up with \"What did you just say?\" Hmm, okay.\n",
      "\n",
      "First, let's break down what happened. The initial request was a straightforward translation from English to French. I provided the correct phrase. Then, the user is asking about that response. They might be testing my ability or perhaps verifying if I understood correctly.\n",
      "\n",
      "I need to consider why they're asking this. Maybe they want confirmation that their previous query was handled properly. Or perhaps they're checking for consistency in the language used in our interactions.\n",
      "\n",
      "Thinking about the context, since it's a friendly exchange, maybe the user is just curious or maybe they made an error themselves and are seeking clarification. Either way, my response should be helpful and positive to maintain a good rapport.\n",
      "\n",
      "In French, \"What did you just say?\" translates naturally as \"Qu'est-ce que tu viens de dire?\" So I'll use that. Adding \"j'espÃ¨re que Ã§a t'a paru compris\" seems appropriate because it's polite and shows that the translation was clear for them. It also keeps the conversation flowing smoothly.\n",
      "\n",
      "I should make sure my response is concise yet informative, matching their query exactly without adding unnecessary details. That way, they get what they're looking for and any follow-up questions can be addressed effectively.\n",
      "</think>\n",
      "\n",
      "Qu'est-ce que tu viens de dire ? J'espÃ¨re que Ã§a t'a paru compris !\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Memory\n",
    "èŠå¤©æ¶ˆæ¯å†å² (ChatMessageHistory)\n",
    "å¤§å¤šæ•°ï¼ˆå¦‚æœä¸æ˜¯å…¨éƒ¨ï¼‰å†…å­˜æ¨¡å—çš„æ ¸å¿ƒå®ç”¨ç±»ä¹‹ä¸€æ˜¯ ChatMessageHistory ç±»ã€‚\n",
    "è¿™æ˜¯ä¸€ä¸ªè¶…è½»é‡çº§çš„åŒ…è£…å™¨ï¼Œå®ƒå…¬å¼€äº†æ–¹ä¾¿çš„æ–¹æ³•æ¥ä¿å­˜äººç±»æ¶ˆæ¯ã€AI æ¶ˆæ¯ï¼Œç„¶åè·å–å®ƒä»¬å…¨éƒ¨ã€‚\n",
    "ConversationBufferMemory\n",
    "ç°åœ¨æˆ‘ä»¬å±•ç¤ºå¦‚ä½•åœ¨é“¾ä¸­ä½¿ç”¨è¿™ä¸ªç®€å•çš„æ¦‚å¿µã€‚\n",
    "æˆ‘ä»¬é¦–å…ˆå±•ç¤º ConversationBufferMemoryï¼Œå®ƒåªæ˜¯ ChatMessageHistory çš„ä¸€ä¸ªåŒ…è£…å™¨ï¼Œå¯ä»¥æå–å˜é‡ä¸­çš„æ¶ˆæ¯ã€‚"
   ],
   "id": "362e47a730ada201"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:22:10.651662Z",
     "start_time": "2025-03-21T09:22:10.643944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"hi!\")\n",
    "history.add_ai_message(\"whats up?\")\n",
    "print(history.messages)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"whats up yo?\")\n",
    "memory.load_memory_variables({})"
   ],
   "id": "75e24d804069e795",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}), AIMessage(content='whats up?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='whats up yo?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "chat chain   ConversationChain",
   "id": "2221a6325ab57f9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=ds_chat,\n",
    "    # verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "conversation.predict(input=\"Hi there!\")"
   ],
   "id": "46d0cedbc38dfbaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "conversation.predict(input=\"What did you just say?\")",
   "id": "62f3e98c53bc318a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "åœ¨ LLMChain ä¸­ä½¿ç”¨ Memory ç±»ã€‚åœ¨æœ¬æ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬å°†æ·»åŠ  ConversationBufferMemory ç±»ï¼Œä½†å®é™…ä¸Šå¯ä»¥ä½¿ç”¨ä»»ä½•è®°å¿†ç±»ã€‚",
   "id": "cc5d901631f23b27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:12:42.937299Z",
     "start_time": "2025-03-21T09:12:40.981137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"],\n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "llm_chain = LLMChain(\n",
    "    llm=ds_chat,\n",
    "    prompt=prompt,\n",
    "    # verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "llm_chain.predict(human_input=\"Hi there my friend\")"
   ],
   "id": "a3ddb070673b253e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user greeted me as \"my friend.\" I should respond warmly to show they feel comfortable. Maybe mention that I\\'m here for help and ask how I can assist them today.\\n</think>\\n\\nHello! My friend, how can I assist you today?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:12:51.480650Z",
     "start_time": "2025-03-21T09:12:45.877191Z"
    }
   },
   "cell_type": "code",
   "source": "llm_chain.predict(human_input=\"Not too bad - how are you?\")",
   "id": "67daf52a5da127bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, so the user greeted me with \"Not too bad - how are you?\" after I responded to their initial greeting. It\\'s a friendly exchange. They seem to be in good spirits and want to know about my well-being.\\n\\nI should acknowledge their comment positively to maintain a pleasant tone. Then, it\\'s time to pivot into offering help since that was the main goal of our conversationâ€”assisting them today.\\n\\nMaybe say something like, \"Thanks for the kind words! I\\'m doing well, thank you. How can I assist you today?\" That sounds natural and inviting.\\n\\nLet me make sure the response is friendly but also professional enough to serve their purpose without being too casual or overly formal.\\n</think>\\n\\nThanks for asking! I\\'m doing well, thanks to you. How can I assist you today?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "llm_chain.memory.buffer",
   "id": "46bffa2c5307f3c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LangGraph\n",
    "The example below shows how to use LangGraph to implement a ConversationChain or LLMChain with ConversationBufferMemory."
   ],
   "id": "432aafaa75c33a48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:52:11.293124Z",
     "start_time": "2025-03-21T09:52:11.190911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import uuid\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = ds_chat.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Adding memory is straight forward in langgraph!\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(\n",
    "    checkpointer=memory\n",
    ")"
   ],
   "id": "2efc63c0418e5f53",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:52:33.527526Z",
     "start_time": "2025-03-21T09:52:19.232394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The thread id is a unique key that identifies\n",
    "# this particular conversation.\n",
    "# We'll just generate a random uuid here.\n",
    "# This enables a single application to manage conversations among multiple users.\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Here, let's confirm that the AI remembers our name!\n",
    "input_message = HumanMessage(content=\"what was my name?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ],
   "id": "fc0b44f1736f07cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hi Bob! How can I assist you today? ğŸ˜Š\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "what was my name?\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "<think>\n",
      "Alright, the user greeted me as \"Bob\" and then asked, \"what was my name?\" \n",
      "\n",
      "I need to figure out how to respond appropriately. Since they already identified themselves as Bob earlier, it's important to clarify that.\n",
      "\n",
      "But wait, in our conversation history, I mentioned \"Hi! I'm Bob.\" So maybe they're confirming their identity or seeking more information about me.\n",
      "\n",
      "They might be curious about my name or prefer to use a different greeting. It could also be a way for them to set the context for future interactions.\n",
      "\n",
      "I should acknowledge that my name is Bob and offer further assistance if needed. That way, I'm providing clarity while inviting them to continue our conversation.\n",
      "</think>\n",
      "\n",
      "It seems like you might be asking about your name or how I got the name \"Bob.\" If this is part of a larger discussion or context, feel free to clarify, and I'll do my best to assist!\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:57:56.111242Z",
     "start_time": "2025-03-21T09:57:55.435949Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "5db9f23cb4080a35",
   "outputs": [],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
